# A headless service to create DNS records.
apiVersion: v1
kind: Service
metadata:
  name: hdfs-datanode
  labels:
    app: hdfs-datanode
{{- block "extraMetadata" . }}
{{- end }}
spec:
  ports:
  - port: 50010
    name: fs
  clusterIP: None
  selector:
    app: hdfs-datanode
---
# A headless service for the web interface.
apiVersion: v1
kind: Service
metadata:
  name: hdfs-datanode-web
  labels:
    app: hdfs-datanode
{{- block "extraMetadata" . }}
{{- end }}
spec:
  ports:
  - port: 50075
    name: web
  selector:
    app: hdfs-datanode
---

apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: hdfs-datanode
  labels:
    app: hdfs-datanode
{{- block "extraMetadata" . }}
{{- end }}
spec:
  serviceName: "hdfs-datanode"
  replicas: {{ .Values.datanode.replicas }}
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: hdfs-datanode
{{- if .Values.datanode.labels }}
{{ toYaml .Values.datanode.labels | indent 8 }}
{{- end }}
      annotations:
        hdfs-con***REMOVED***gmap-hash: {{ include (print $.Template.BasePath "/con***REMOVED***gmap.yaml") . | sha256sum }}
{{- if .Values.datanode.annotations }}
{{ toYaml .Values.datanode.annotations | indent 8 }}
{{- end }}
    spec:
      terminationGracePeriodSeconds: {{ .Values.datanode.terminationGracePeriodSeconds }}
{{- if .Values.securityContext }}
      securityContext:
{{ toYaml .Values.securityContext | indent 8 }}
{{- end }}
{{- if .Values.datanode.af***REMOVED***nity }}
      af***REMOVED***nity:
{{ toYaml .Values.datanode.af***REMOVED***nity | indent 8 }}
{{- end }}
      initContainers:
      # wait-for-namenode exists because for some reason the datanode is unable
      # to connect to the namenode if it starts before the namenode's DNS name
      # is resolvable. We cannot use a clusterIP service for the namenode which
      # would always be resolvable, because on Openshift, clusterIP services
      # NAT loses sourceIPs, breaking HDFS clustering.
      - name: wait-for-namenode
        image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
        imagePullPolicy: {{ .Values.image.pullPolicy }}
        command:
        - '/bin/bash'
        - '-c'
        - 'until host $NAMENODE_HOST; do echo waiting for $NAMENODE_HOST; sleep 2; done;'
        resources:
          requests:
            memory: "5Mi"
            cpu: "10m"
          limits:
            memory: "50Mi"
            cpu: "50m"
        volumeMounts:
        - name: hdfs-datanode-data
          mountPath: /hadoop/dfs/data
          # we use a subPath to avoid the lost+found directory at the root of
          # the volume effecting the hdfs formating
          subPath: hadoop/dfs/data
        # required for openshift
        - name: namenode-empty
          mountPath: /hadoop/dfs/name
        env:
        - name: NAMENODE_HOST
          valueFrom:
            con***REMOVED***gMapKeyRef:
              name: hdfs-con***REMOVED***g
              key: namenode-host
      containers:
      - name: hdfs-datanode
        image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
        imagePullPolicy: {{ .Values.image.pullPolicy }}
        args: ["datanode-entrypoint.sh"]
        env:
        - name: CORE_CONF_fs_defaultFS
          valueFrom:
            con***REMOVED***gMapKeyRef:
              name: hdfs-con***REMOVED***g
              key: default-fs
        - name: HDFS_CONF_dfs_datanode_data_dir_perm
          valueFrom:
            con***REMOVED***gMapKeyRef:
              name: hdfs-con***REMOVED***g
              key: datanode-data-dir-perms
        - name: HADOOP_LOGLEVEL
          valueFrom:
            con***REMOVED***gMapKeyRef:
              name: hdfs-con***REMOVED***g
              key: log-level
        - name: MY_NODE_NAME
          valueFrom:
            ***REMOVED***eldRef:
              ***REMOVED***eldPath: spec.nodeName
        - name: MY_POD_NAME
          valueFrom:
            ***REMOVED***eldRef:
              ***REMOVED***eldPath: metadata.name
        - name: MY_POD_NAMESPACE
          valueFrom:
            ***REMOVED***eldRef:
              ***REMOVED***eldPath: metadata.namespace
        - name: MY_MEM_REQUEST
          valueFrom:
            resourceFieldRef:
              containerName: hdfs-datanode
              resource: requests.memory
        - name: MY_MEM_LIMIT
          valueFrom:
            resourceFieldRef:
              containerName: hdfs-datanode
              resource: limits.memory
        - name: JAVA_MAX_MEM_RATIO
          value: "50"
        ports:
        - containerPort: 50010
          name: fs
        - containerPort: 50075
          name: http
        volumeMounts:
        - name: hdfs-datanode-data
          mountPath: /hadoop/dfs/data
          # we use a subPath to avoid the lost+found directory at the root of
          # the volume effecting the hdfs formating
          subPath: hadoop/dfs/data
        # required for openshift
        - name: namenode-empty
          mountPath: /hadoop/dfs/name
        livenessProbe:
          exec:
            command:
            - check-datanode-healthy.sh
          initialDelaySeconds: 90
          periodSeconds: 30
          failureThreshold: 2
          successThreshold: 1
        resources:
{{ toYaml .Values.datanode.resources | indent 10 }}
      serviceAccount: hdfs
{{- if .Values.imagePullSecrets }}
      imagePullSecrets:
{{ toYaml .Values.imagePullSecrets | indent 8 }}
{{- end }}
      volumes:
      - name: namenode-empty
        emptyDir: {}
  volumeClaimTemplates:
  - metadata:
      name: "hdfs-datanode-data"
      labels:
        app: hdfs-datanode
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: {{ .Values.datanode.storage.class }}
      resources:
        requests:
          storage: {{ .Values.datanode.storage.size }}
