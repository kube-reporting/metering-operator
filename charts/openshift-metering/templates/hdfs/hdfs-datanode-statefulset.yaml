# A headless service to create DNS records.
apiVersion: v1
kind: Service
metadata:
  name: hdfs-datanode
  labels:
    app: hdfs-datanode
    hdfs: datanode
spec:
  ports:
  - port: 9866
    name: fs
  clusterIP: None
  selector:
    app: hdfs-datanode
---
# A headless service for the web interface.
apiVersion: v1
kind: Service
metadata:
  name: hdfs-datanode-web
  labels:
    app: hdfs-datanode
    hdfs: datanode
spec:
  ports:
  - port: 9864
    name: web
  selector:
    app: hdfs-datanode
    hdfs: datanode
---

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: hdfs-datanode
  labels:
    app: hdfs-datanode
    hdfs: datanode
spec:
  serviceName: "hdfs-datanode"
  replicas: {{ .Values.hdfs.spec.datanode.replicas }}
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app: hdfs-datanode
      hdfs: datanode
{{- if .Values.hdfs.spec.datanode.labels }}
{{ toYaml .Values.hdfs.spec.datanode.labels | indent 6 }}
{{- end }}
  template:
    metadata:
      labels:
        app: hdfs-datanode
        hdfs: datanode
{{- if .Values.hdfs.spec.datanode.labels }}
{{ toYaml .Values.hdfs.spec.datanode.labels | indent 8 }}
{{- end }}
      annotations:
        hdfs-con***REMOVED***gmap-hash: {{ include (print $.Template.BasePath "/hdfs/hdfs-con***REMOVED***gmap.yaml") . | sha256sum }}
{{- if .Values.hdfs.spec.datanode.annotations }}
{{ toYaml .Values.hdfs.spec.datanode.annotations | indent 8 }}
{{- end }}
    spec:
      terminationGracePeriodSeconds: {{ .Values.hdfs.spec.datanode.terminationGracePeriodSeconds }}
{{- if .Values.hdfs.spec.securityContext }}
      securityContext:
{{ toYaml .Values.hdfs.spec.securityContext | indent 8 }}
{{- end }}
{{- if .Values.hdfs.spec.datanode.af***REMOVED***nity }}
      af***REMOVED***nity:
{{ toYaml .Values.hdfs.spec.datanode.af***REMOVED***nity | indent 8 }}
{{- end }}
{{- if .Values.hdfs.spec.datanode.nodeSelector }}
      nodeSelector:
{{ toYaml .Values.hdfs.spec.datanode.nodeSelector | indent 8 }}
{{- end }}
{{- if .Values.hdfs.spec.datanode.tolerations }}
      tolerations:
{{ toYaml .Values.hdfs.spec.datanode.tolerations | indent 8 }}
{{- end }}
      initContainers:
      # wait-for-namenode exists because for some reason the datanode is unable
      # to connect to the namenode if it starts before the namenode's DNS name
      # is resolvable. We cannot use a clusterIP service for the namenode which
      # would always be resolvable, because on Openshift, clusterIP services
      # NAT loses sourceIPs, breaking HDFS clustering.
      - name: wait-for-namenode
        image: "{{ .Values.hdfs.spec.image.repository }}:{{ .Values.hdfs.spec.image.tag }}"
        imagePullPolicy: {{ .Values.hdfs.spec.image.pullPolicy }}
        command:
        - '/bin/bash'
        - '-c'
        - 'until host $NAMENODE_HOST; do echo waiting for $NAMENODE_HOST; sleep 2; done;'
        resources:
          requests:
            memory: "5Mi"
            cpu: "10m"
          limits:
            memory: "50Mi"
            cpu: "50m"
        volumeMounts:
        - name: hdfs-datanode-data
          mountPath: /hadoop/dfs/data
          # we use a subPath to avoid the lost+found directory at the root of
          # the volume effecting the hdfs formating
          subPath: hadoop/dfs/data
        # required for openshift
        - name: namenode-empty
          mountPath: /hadoop/dfs/name
        env:
        - name: NAMENODE_HOST
          valueFrom:
            con***REMOVED***gMapKeyRef:
              name: hdfs-con***REMOVED***g
              key: namenode-host
      containers:
      - name: hdfs-datanode
        image: "{{ .Values.hdfs.spec.image.repository }}:{{ .Values.hdfs.spec.image.tag }}"
        imagePullPolicy: {{ .Values.hdfs.spec.image.pullPolicy }}
        command: ["/hadoop-con***REMOVED***g/entrypoint.sh"]
        args: ["/hadoop-con***REMOVED***g/datanode-entrypoint.sh"]
        env:
        - name: HADOOP_LOGLEVEL
          valueFrom:
            con***REMOVED***gMapKeyRef:
              name: hdfs-con***REMOVED***g
              key: log-level
        - name: MY_NODE_NAME
          valueFrom:
            ***REMOVED***eldRef:
              ***REMOVED***eldPath: spec.nodeName
        - name: MY_POD_NAME
          valueFrom:
            ***REMOVED***eldRef:
              ***REMOVED***eldPath: metadata.name
        - name: MY_POD_NAMESPACE
          valueFrom:
            ***REMOVED***eldRef:
              ***REMOVED***eldPath: metadata.namespace
        - name: MY_MEM_REQUEST
          valueFrom:
            resourceFieldRef:
              containerName: hdfs-datanode
              resource: requests.memory
        - name: MY_MEM_LIMIT
          valueFrom:
            resourceFieldRef:
              containerName: hdfs-datanode
              resource: limits.memory
        - name: JAVA_MAX_MEM_RATIO
          value: "50"
        ports:
        - containerPort: 9864
          name: http
        - containerPort: 9866
          name: fs
        - containerPort: 9867
          name: ipc
        volumeMounts:
        - name: hdfs-con***REMOVED***g
          mountPath: /hadoop-con***REMOVED***g
        - name: hdfs-datanode-data
          mountPath: /hadoop/dfs/data
          # we use a subPath to avoid the lost+found directory at the root of
          # the volume effecting the hdfs formating
          subPath: hadoop/dfs/data
        # required for openshift
        - name: namenode-empty
          mountPath: /hadoop/dfs/name
        - name: hadoop-logs
          mountPath: /opt/hadoop/logs
        livenessProbe:
          exec:
            command: ["/hadoop-con***REMOVED***g/check-datanode-healthy.sh" ]
          initialDelaySeconds: 90
          periodSeconds: 30
          failureThreshold: 2
          successThreshold: 1
        resources:
{{ toYaml .Values.hdfs.spec.datanode.resources | indent 10 }}
      serviceAccount: hdfs
{{- if .Values.hdfs.spec.imagePullSecrets }}
      imagePullSecrets:
{{ toYaml .Values.hdfs.spec.imagePullSecrets | indent 8 }}
{{- end }}
      volumes:
      - name: hdfs-con***REMOVED***g
        con***REMOVED***gMap:
          name: hdfs-con***REMOVED***g
          defaultMode: 0555
      - name: namenode-empty
        emptyDir: {}
      - name: hadoop-logs
        emptyDir: {}
  volumeClaimTemplates:
  - metadata:
      name: "hdfs-datanode-data"
      labels:
        app: hdfs-datanode
        hdfs: datanode
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: {{ .Values.hdfs.spec.datanode.storage.class }}
      resources:
        requests:
          storage: {{ .Values.hdfs.spec.datanode.storage.size }}
