apiVersion: v1
kind: Con***REMOVED***gMap
metadata:
  name: hive-scripts
{{- block "extraMetadata" . }}
{{- end }}
data:
  entrypoint.sh: |
    #!/bin/bash
    set -e

    max_memory() {
        local memory_limit=$1
        local ratio=${JAVA_MAX_MEM_RATIO:-50}
        echo "${memory_limit} ${ratio} 1048576" | awk '{printf "%d\n" , ($1*$2)/(100*$3) + 0.5}'
    }

    # Check for container memory limits/request and use it to set JVM Heap size.
    # Defaults to 50% of the limit/request value.
    if [ -n "$MY_MEM_LIMIT" ]; then
        export HADOOP_HEAPSIZE="$( max_memory $MY_MEM_LIMIT )"
    elif [ -n "$MY_MEM_REQUEST" ]; then
        export HADOOP_HEAPSIZE="$( max_memory $MY_MEM_REQUEST )"
    ***REMOVED***

    if [ -z "$HADOOP_HEAPSIZE" ]; then
        echo "Unable to automatically set HADOOP_HEAPSIZE"
    ***REMOVED***
        echo "Setting HADOOP_HEAPSIZE to ${HADOOP_HEAPSIZE}M"
    ***REMOVED***

    # add UID to /etc/passwd if missing
    if ! whoami &> /dev/null; then
        if [ -w /etc/passwd ]; then
            echo "Adding user ${USER_NAME:-hadoop} with current UID $(id -u) to /etc/passwd"
            # Remove existing entry with user ***REMOVED***rst.
            # cannot use sed -i because we do not have permission to write new
            # ***REMOVED***les into /etc
            sed  "/${USER_NAME:-hadoop}:x/d" /etc/passwd > /tmp/passwd
            # add our user with our current user ID into passwd
            echo "${USER_NAME:-hadoop}:x:$(id -u):0:${USER_NAME:-hadoop} user:${HOME}:/sbin/nologin" >> /tmp/passwd
            # overwrite existing contents with new contents (cannot replace the
            # ***REMOVED***le due to permissions)
            cat /tmp/passwd > /etc/passwd
            rm /tmp/passwd
        ***REMOVED***
    ***REMOVED***

    # symlink our con***REMOVED***guration ***REMOVED***les to the correct location
    if [ -f /hadoop-con***REMOVED***g/core-site.xml ]; then
      ln -s -f /hadoop-con***REMOVED***g/core-site.xml /etc/hadoop/core-site.xml
    ***REMOVED***
      echo "/hadoop-con***REMOVED***g/core-site.xml doesnt exist, skipping symlink"
    ***REMOVED***
    if [ -f /hadoop-con***REMOVED***g/hdfs-site.xml ]; then
      ln -s -f /hadoop-con***REMOVED***g/hdfs-site.xml /etc/hadoop/hdfs-site.xml
    ***REMOVED***
      echo "/hadoop-con***REMOVED***g/hdfs-site.xml doesnt exist, skipping symlink"
    ***REMOVED***
    ln -s -f /hive-con***REMOVED***g/hive-site.xml $HIVE_HOME/conf/hive-site.xml
    ln -s -f /hive-con***REMOVED***g/hive-log4j2.properties $HIVE_HOME/conf/hive-log4j2.properties
    ln -s -f /hive-con***REMOVED***g/hive-exec-log4j2.properties $HIVE_HOME/conf/hive-exec-log4j2.properties

    # Set garbage collection settings
    export GC_SETTINGS="-XX:+UseG1GC -XX:G1HeapRegionSize=32M -XX:+UseGCOverheadLimit -XX:+ExplicitGCInvokesConcurrent -XX:+HeapDumpOnOutOfMemoryError -XX:+ExitOnOutOfMemoryError"
    # Set JMX options
    export JMX_OPTIONS="-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=1026"
    # Set garbage collection logs
    export HADOOP_LOG_DIR="${HADOOP_HOME}/logs"
    export GC_SETTINGS="${GC_SETTINGS} -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:${HADOOP_LOG_DIR}/gc.log"

    export HIVE_LOGLEVEL="${HIVE_LOGLEVEL:-INFO}"
    export HADOOP_OPTS="${HADOOP_OPTS} ${GC_SETTINGS} ${JMX_OPTIONS}"
    export HIVE_METASTORE_HADOOP_OPTS=" -Dhive.log.level=${HIVE_LOGLEVEL} "
    export HIVE_OPTS="${HIVE_OPTS} --hiveconf hive.root.logger=${HIVE_LOGLEVEL},console "

    exec $@
