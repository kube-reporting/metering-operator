apiVersion: v1
kind: Con***REMOVED***gMap
metadata:
  name: hdfs-con***REMOVED***g
{{- block "extraMetadata" . }}
{{- end }}
data:
  log-level: {{ upper .Values.spec.con***REMOVED***g.logLevel | quote }}
  namenode-host: {{ .Values.spec.con***REMOVED***g.namenodeHost }}
  hdfs-site.xml: |
    <con***REMOVED***guration>
      <property>
        <name>dfs.permissions.enabled</name>
        <value>false</value>
      </property>
      <property>
        <name>dfs.webhdfs.enabled</name>
        <value>true</value>
      </property>
      <property>
        <name>dfs.namenode.name.dir</name>
        <value>***REMOVED***le:///hadoop/dfs/name</value>
      </property>
      <property>
        <name>dfs.namenode.rpc-bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <property>
        <name>dfs.namenode.servicerpc-bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <property>
        <name>dfs.namenode.http-bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <property>
        <name>dfs.namenode.https-bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <property>
        <name>dfs.client.use.datanode.hostname</name>
        <value>true</value>
      </property>
      <property>
        <name>dfs.datanode.use.datanode.hostname</name>
        <value>true</value>
      </property>
      <property>
        <name>dfs.datanode.data.dir</name>
        <value>***REMOVED***le:///hadoop/dfs/data</value>
      </property>
      <property>
        <name>dfs.datanode.data.dir.perm</name>
        <value>{{ .Values.spec.con***REMOVED***g.datanodeDataDirPerms }}</value>
      </property>
      <property>
        <name>dfs.replication</name>
        <value>{{ .Values.spec.con***REMOVED***g.replicationFactor }}</value>
      </property>
      <property>
        <name>net.topology.script.***REMOVED***le.name</name>
        <value>/hadoop-con***REMOVED***g/topology-con***REMOVED***guration.sh</value>
      </property>
    </con***REMOVED***guration>
  core-site.xml: |
    <con***REMOVED***guration>
      <property>
        <name>hadoop.proxyuser.hue.hosts</name>
        <value>*</value>
      </property>
      <property>
        <name>hadoop.proxyuser.hue.groups</name>
        <value>*</value>
      </property>
      <property>
        <name>hadoop.http.staticuser.user</name>
        <value>root</value>
      </property>
      <property>
          <name>fs.defaultFS</name>
          <value>{{ .Values.spec.con***REMOVED***g.defaultFS }}</value>
      </property>
    </con***REMOVED***guration>
  entrypoint.sh: |
    #!/bin/bash -e

    max_memory() {
        local memory_limit=$1
        local ratio=${JAVA_MAX_MEM_RATIO:-50}
        echo "${memory_limit} ${ratio} 1048576" | awk '{printf "%d\n" , ($1*$2)/(100*$3) + 0.5}'
    }

    # Check for container memory limits/request and use it to set JVM Heap size.
    # Defaults to 50% of the limit/request value.
    if [ -n "$MY_MEM_LIMIT" ]; then
        export HADOOP_HEAPSIZE="$( max_memory $MY_MEM_LIMIT )"
    elif [ -n "$MY_MEM_REQUEST" ]; then
        export HADOOP_HEAPSIZE="$( max_memory $MY_MEM_REQUEST )"
    ***REMOVED***

    if [ -z "$HADOOP_HEAPSIZE" ]; then
        echo "Unable to automatically set HADOOP_HEAPSIZE"
    ***REMOVED***
        echo "Setting HADOOP_HEAPSIZE to ${HADOOP_HEAPSIZE}M"
    ***REMOVED***

    # Set garbage collection settings
    export GC_SETTINGS="-XX:+UseG1GC -XX:G1HeapRegionSize=32M -XX:+UseGCOverheadLimit -XX:+ExplicitGCInvokesConcurrent -XX:+HeapDumpOnOutOfMemoryError -XX:+ExitOnOutOfMemoryError -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps"
    # Set JMX options
    export JMX_OPTIONS="-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=1026"
    # Set garbage collection logs
    export GC_SETTINGS="${GC_SETTINGS} -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"

    # set name node options
    export HDFS_NAMENODE_OPTS="${HDFS_NAMENODE_OPTS} -Dhadoop.security.logger=INFO,RFAS ${GC_SETTINGS} ${JMX_OPTIONS}"
    # set datanode options
    export HDFS_DATANODE_OPTS="${HDFS_DATANODE_OPTS} -Dhadoop.security.logger=ERROR,RFAS ${GC_SETTINGS} ${JMX_OPTIONS}"

    # add UID to /etc/passwd if missing
    if ! whoami &> /dev/null; then
      if [ -w /etc/passwd ]; then
        echo "${USER_NAME:-hadoop}:x:$(id -u):0:${USER_NAME:-hadoop} user:${HOME}:/sbin/nologin" >> /etc/passwd
      ***REMOVED***
    ***REMOVED***

    # symlink our con***REMOVED***guration ***REMOVED***les to the correct location
    ln -s -f /hadoop-con***REMOVED***g/core-site.xml /etc/hadoop/core-site.xml
    ln -s -f /hadoop-con***REMOVED***g/hdfs-site.xml /etc/hadoop/hdfs-site.xml

    exec $@
  namenode-entrypoint.sh: |
    #!/bin/bash

    namedir=/hadoop/dfs/name
    if [ ! -d "$namedir" ]; then
      echo "Namenode name directory not found: $namedir"
      exit 2
    ***REMOVED***

    if [ -z "$CLUSTER_NAME" ]; then
      echo "Cluster name not speci***REMOVED***ed"
      exit 2
    ***REMOVED***

    if [ "$(ls -A $namedir)" == "" ]; then
      echo "Formatting namenode name directory: $namedir"
      hdfs --con***REMOVED***g "$HADOOP_CONF_DIR" namenode -format "$CLUSTER_NAME"
    ***REMOVED***

    exec hdfs --con***REMOVED***g "$HADOOP_CONF_DIR" namenode "$@"
  datanode-entrypoint.sh: |
    #!/bin/bash

    datadir=/hadoop/dfs/data
    if [ ! -d "$datadir" ]; then
      echo "Datanode data directory not found: $datadir"
      exit 2
    ***REMOVED***

    exec hdfs --con***REMOVED***g "$HADOOP_CONF_DIR" datanode "$@"


  check-datanode-healthy.sh: |
    #!/bin/bash

    : "${DATANODE_ADDRESS:=127.0.0.1:9864}"

    set -ex

    if [ "$(curl "$DATANODE_ADDRESS/jmx?qry=Hadoop:service=DataNode,name=DataNodeInfo" | jq '.beans[0].NamenodeAddresses' -r | jq 'to_entries | map(.value) | all')" == "true" ]; then
        echo "Name node addresses all have addresses, healthy"
        exit 0
    ***REMOVED***
        echo "found null namenode addresses in JMX metrics, unhealthy"
        exit 1
    ***REMOVED***

  topology-con***REMOVED***guration.sh: |
    #!/bin/bash
    # taken from https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/RackAwareness.html
    # With this network topology, we are treating each datanode as a rack by using
    # the podIP as the rack name.
    # 1) 'echo $@' will echo all ARGV values to xargs.
    # 2) 'xargs' will enforce that we print a single argv value per line
    # 3) 'printf' will add the '/rack-' pre***REMOVED***x to the podIP.
    echo $@ | xargs -n1 printf '/rack-%s\n'

  ***REMOVED***x-under-replicated-***REMOVED***les.sh: |
    #!/bin/bash
    # based on
    # https://community.hortonworks.com/articles/4427/***REMOVED***x-under-replicated-blocks-in-hdfs-manually.html
    #
    # this script is intended to be run manually after changing the
    # dfs.replication value or after scaling up hdfs datanodes and forcing
    # under replicated ***REMOVED***les to have the correct replication value.
    # takes two arguments, the replication factor, and optionally true/false
    # to indicate if it should wait for each ***REMOVED***le to replicate.
    set -e
    REPLICATION_FACTOR="$1"
    WAIT="$2"
    if [ -z "$REPLICATION_FACTOR" ]; then
      echo "Usage: $0 replication_factor [wait=true/false]"
      exit 1
    ***REMOVED***
    rm -f /tmp/under_replicated_***REMOVED***les
    touch /tmp/under_replicated_***REMOVED***les
    echo "Running hdfs fsck to check for under replicated ***REMOVED***les"
    hdfs fsck / > /tmp/fsck.log
    # example output:
    # /operator_metering/storage/datasource_node_allocatable_cpu_cores/20181016_210834_02359_srzkh_40207cb6-56fb-4aad-9428-acb203250be8:  Under replicated BP-27232867-172.16.2.102-1539711209651:blk_1073742397_1573. Target Replicas is 3 but found 1 live replica(s), 0 decommissioned replica(s), 0 decommissioning replica(s).
    # /operator_metering/storage/scheduled_report_namespace_cpu_usage_daily/20181111_035359_00063_yihp2_885f5ff5-45dc-4a3e-acb1-0a39f7aff8ab:  Replica placement policy is violated for BP-27232867-172.16.2.102-1539711209651:blk_1073843950_103126. Block should be additionally replicated on 1 more rack(s). Total number of racks in the cluster: 3
    UNDER_REP_REGEX='^(.*):[[:space:]]+Under replicated.*Target Replicas is ([0-9]+) but found ([0-9]) live.*'
    NEEDS_REP_REGEX='^(.*):[[:space:]]+Replica placement policy is violated.*'
    echo "Checking for ***REMOVED***les under replicated ***REMOVED***les. Replication factor: $REPLICATION_FACTOR"
    while read line; do
      if [[ $line =~ $UNDER_REP_REGEX ]]; then
        HDFS_FILE="${BASH_REMATCH[1]}"
        FILE_REP="${BASH_REMATCH[2]}"
        LIVE_REPS="${BASH_REMATCH[3]}"
        # ***REMOVED***rst check if the replication factor is set correct for the ***REMOVED***le
        if [ "$FILE_REP" != "$REPLICATION_FACTOR" ]; then
          echo "$HDFS_FILE" >> /tmp/under_replicated_***REMOVED***les
        ***REMOVED***
      # check for ***REMOVED***les which have replication set to the correct value but
      # don't actually have the target number of replicas.
      elif [[ "$WAIT" == "true" && $line =~ $NEEDS_REP_REGEX ]]; then
          HDFS_FILE="${BASH_REMATCH[1]}"
          echo "$HDFS_FILE" >> /tmp/under_replicated_***REMOVED***les
      ***REMOVED***
    done < /tmp/fsck.log

    # setup args for hdfs fs command
    ARGS=(-setrep)
    if [ "$WAIT" == "true" ]; then
      ARGS+=(-w)
    ***REMOVED***
    ARGS+=($REPLICATION_FACTOR)
    echo "Running hdfs fs to set replication"
    while read hdfs***REMOVED***le; do
      echo "Fixing $hdfs***REMOVED***le :"
      hadoop fs "${ARGS[@]}" "$hdfs***REMOVED***le"
    done < /tmp/under_replicated_***REMOVED***les
    echo "Done"
