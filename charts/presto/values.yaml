spec:
  presto:
    image:
      repository: quay.io/openshift/origin-metering-presto
      tag: latest
      pullPolicy: Always

    securityContext:
      runAsNonRoot: true

    con***REMOVED***g:
      discoveryURI: http://presto:8080
      environment: production
      # maxQueryLength: 1000000
      hiveMetastoreURI: thrift://hive-metastore:9083
      nodeSchedulerIncludeCoordinator: true
      metastoreTimeout: null

      connectors:
        extraConnectorFiles: []

    coordinator:
      terminationGracePeriodSeconds: 30
      con***REMOVED***g:
        logLevel: info
        taskMaxWorkerThreads: null
        taskMinDrivers: null
        jvm:
          G1HeapRegionSize: null
          parallelGCThreads: null
          concGCThreads: null
          permSize: null
          maxGcPauseMillis: 200
          initiatingHeapOccupancyPercent: null
          # use 50% of our memory limit by default for java heap size
          percentMemoryLimitAsHeap: "50"
          extraFlags: []

      resources:
        requests:
          memory: "2Gi"
          cpu: "2"
        limits:
          memory: "2Gi"
          cpu: "2"

      nodeSelector: {}
      tolerations: []

      af***REMOVED***nity:
        podAntiAf***REMOVED***nity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - presto
            topologyKey: "kubernetes.io/hostname"

    worker:
      replicas: 0
      terminationGracePeriodSeconds: 30
      con***REMOVED***g:
        logLevel: info
        taskMaxWorkerThreads: null
        taskMinDrivers: null
        jvm:
          G1HeapRegionSize: null
          parallelGCThreads: null
          concGCThreads: null
          permSize: null
          maxGcPauseMillis: null
          initiatingHeapOccupancyPercent: null
          percentMemoryLimitAsHeap: "50"
          extraFlags: []

      resources:
        requests:
          memory: "2Gi"
          cpu: "2"
        limits:
          memory: "2Gi"
          cpu: "2"

      nodeSelector: {}
      tolerations: []

      af***REMOVED***nity:
        podAntiAf***REMOVED***nity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - presto
            topologyKey: "kubernetes.io/hostname"

    labels: {}
    annotations: {}

  hive:
    image:
      repository: quay.io/openshift/origin-metering-hive
      tag: latest
      pullPolicy: Always

    con***REMOVED***g:
      defaultfs: null
      dbConnectionURL: "jdbc:derby:;databaseName=/var/lib/hive/data;create=true"
      dbConnectionDriver: "org.apache.derby.jdbc.EmbeddedDriver"
      dbConnectionUsername: null
      dbConnectionPassword: null
      enableMetastoreSchemaVeri***REMOVED***cation: false
      autoCreateMetastoreSchema: true
      defaultFileFormat: "orc"
      defaultCompression: "zlib"
      metastoreURIs: "thrift://hive-metastore:9083"
      useHdfsCon***REMOVED***gMap: true
      hdfsCon***REMOVED***gMapName: hdfs-con***REMOVED***g
      metastoreClientSocketTimeout: null

    securityContext:
      runAsNonRoot: true

    terminationGracePeriodSeconds: 30

    metastore:
      con***REMOVED***g:
        logLevel: info
        jvm:
          percentMemoryLimitAsHeap: "50"

      resources:
        requests:
          memory: "650Mi"
          cpu: "500m"
        limits:
          memory: "650Mi"
          cpu: "500m"
      storage:
        create: true
        # Default to null, which means using the default storage class if the
        # defaultStorageClass admission plugin is turned on (is by default on
        # Tectonic).
        class: null
        size: "5Gi"

      readinessProbe:
        initialDelaySeconds: 60
        timeoutSeconds: 15
        periodSeconds: 20
        successThreshold: 1
        failureThreshold: 3
        tcpSocket:
          port: 9083
      livenessProbe:
        initialDelaySeconds: 90
        timeoutSeconds: 15
        periodSeconds: 30
        successThreshold: 1
        failureThreshold: 3
        tcpSocket:
          port: 9083

      nodeSelector: {}
      tolerations: []
      af***REMOVED***nity: {}

    server:
      con***REMOVED***g:
        logLevel: info
        jvm:
          percentMemoryLimitAsHeap: "50"

      resources:
        requests:
          memory: "500Mi"
          cpu: "100m"
        limits:
          memory: "500Mi"
          cpu: "100m"

      readinessProbe:
        initialDelaySeconds: 60
        timeoutSeconds: 15
        periodSeconds: 20
        successThreshold: 1
        failureThreshold: 3
        tcpSocket:
          port: 10000
      livenessProbe:
        # hive server depends on hive metastore so give it more time
        initialDelaySeconds: 300
        timeoutSeconds: 15
        periodSeconds: 30
        successThreshold: 1
        failureThreshold: 3
        tcpSocket:
          port: 10000

      nodeSelector: {}
      tolerations: []
      af***REMOVED***nity: {}

    labels: {}
    annotations: {}

  con***REMOVED***g:
    awsAccessKeyID: ""
    awsSecretAccessKey: ""
    createAwsCredentialsSecret: true
    awsCredentialsSecretName: presto-aws-credentials

    sharedVolume:
      enabled: false
      createPVC: true
      persistentVolumeClaimName: hive-warehouse-data
      mountPath: /user/hive/warehouse
      storage:
        persistentVolumeClaimStorageClass: null
        persistentVolumeClaimSize: "5Gi"
