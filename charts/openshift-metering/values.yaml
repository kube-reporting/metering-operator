global:
  ownerReferences: []

unsupportedFeatures:
  enableHDFS: false

storage:
  type: "hive"
  hive: {}

  # Below is an example of what configuring storage could look like:
  #
  # hive:
  #   type: "hdfs"
  #   hdfs:
  #     namenode: "hdfs-namenode-0.hdfs-namenode:9820"

  # hive:
  #   type: "s3"
  #   s3:
  #     bucket: "bucketname/path/"
  #     awsCredentialsSecretName: "my-aws-secret"

  # hive:
  #   type: "sharedPVC"
  #   sharedPVC:
  #     claimName: "metering-nfs"
  #     createPVC: true
  #     storageClass: null
  #     size: 5Gi
  #     mountPath: "/"

tls:
  enabled: false

permissions:
  meteringAdmins: []
  meteringViewers: []
  reportExporters: []
  reportingAdmins: []
  reportingViewers: []

monitoring:
  createRBAC: true
  enabled: true
  namespace: openshift-monitoring

openshift-reporting:
  enabled: true
  spec:
    defaultStorageLocation:
      create: true
      isDefault: true
      name: hive
      type: hive
      hive:
        databaseName: metering
        unmanagedDatabase: false
        location: ""

    awsBillingReportDataSource:
      enabled: false

    defaultReportDataSources:
      cluster-cpu-capacity-raw:
        spec:
          reportQueryView:
            queryName: cluster-cpu-capacity-raw
      cluster-cpu-usage-raw:
        spec:
          reportQueryView:
            queryName: cluster-cpu-usage-raw
      cluster-memory-capacity-raw:
        spec:
          reportQueryView:
            queryName: cluster-memory-capacity-raw
      cluster-memory-usage-raw:
        spec:
          reportQueryView:
            queryName: cluster-memory-usage-raw
      node-allocatable-cpu-cores:
        spec:
          prometheusMetricsImporter:
            query: |
              kube_node_status_allocatable_cpu_cores * on(node) group_left(provider_id) max(kube_node_info) by (node, provider_id)
      node-allocatable-memory-bytes:
        spec:
          prometheusMetricsImporter:
            query: |
              kube_node_status_allocatable_memory_bytes * on(node) group_left(provider_id) max(kube_node_info) by (node, provider_id)
      node-capacity-cpu-cores:
        spec:
          prometheusMetricsImporter:
            query: |
              kube_node_status_capacity_cpu_cores * on(node) group_left(provider_id) max(kube_node_info) by (node, provider_id)
      node-capacity-memory-bytes:
        spec:
          prometheusMetricsImporter:
            query: |
              kube_node_status_capacity_memory_bytes * on(node) group_left(provider_id) max(kube_node_info) by (node, provider_id)
      node-cpu-allocatable-raw:
        spec:
          reportQueryView:
            queryName: node-cpu-allocatable-raw
      node-cpu-capacity-raw:
        spec:
          reportQueryView:
            queryName: node-cpu-capacity-raw
      node-memory-allocatable-raw:
        spec:
          reportQueryView:
            queryName: node-memory-allocatable-raw
      node-memory-capacity-raw:
        spec:
          reportQueryView:
            queryName: node-memory-capacity-raw
      persistentvolumeclaim-capacity-bytes:
        spec:
          prometheusMetricsImporter:
            query: |
              kubelet_volume_stats_capacity_bytes
      persistentvolumeclaim-capacity-raw:
        spec:
          reportQueryView:
            queryName: persistentvolumeclaim-capacity-raw
      persistentvolumeclaim-phase:
        spec:
          prometheusMetricsImporter:
            query: |
              kube_persistentvolumeclaim_status_phase
      persistentvolumeclaim-phase-raw:
        spec:
          reportQueryView:
            queryName: persistentvolumeclaim-phase-raw
      persistentvolumeclaim-request-bytes:
        spec:
          prometheusMetricsImporter:
            query: |
              max(kube_persistentvolumeclaim_resource_requests_storage_bytes) by (namespace, persistentvolumeclaim) + on (namespace, persistentvolumeclaim) group_left(storageclass, volumename) sum(kube_persistentvolumeclaim_info) by (namespace, persistentvolumeclaim, storageclass, volumename) * 0
      persistentvolumeclaim-request-raw:
        spec:
          reportQueryView:
            queryName: persistentvolumeclaim-request-raw
      persistentvolumeclaim-usage-bytes:
        spec:
          prometheusMetricsImporter:
            query: |
              kubelet_volume_stats_used_bytes
      persistentvolumeclaim-usage-raw:
        spec:
          reportQueryView:
            queryName: persistentvolumeclaim-usage-raw
      persistentvolumeclaim-usage-with-phase-raw:
        spec:
          reportQueryView:
            queryName: persistentvolumeclaim-usage-with-phase-raw
      pod-cpu-request-raw:
        spec:
          reportQueryView:
            queryName: pod-cpu-request-raw
      pod-cpu-usage-raw:
        spec:
          reportQueryView:
            queryName: pod-cpu-usage-raw
      pod-limit-cpu-cores:
        spec:
          prometheusMetricsImporter:
            query: |
              sum(kube_pod_container_resource_limits_cpu_cores) by (pod, namespace, node)
      pod-limit-memory-bytes:
        spec:
          prometheusMetricsImporter:
            query: |
              sum(kube_pod_container_resource_limits_memory_bytes) by (pod, namespace, node)
      pod-memory-request-raw:
        spec:
          reportQueryView:
            queryName: pod-memory-request-raw
      pod-memory-usage-raw:
        spec:
          reportQueryView:
            queryName: pod-memory-usage-raw
      pod-persistentvolumeclaim-request-info:
        spec:
          prometheusMetricsImporter:
            query: |
              kube_pod_spec_volumes_persistentvolumeclaims_info
      pod-request-cpu-cores:
        spec:
          prometheusMetricsImporter:
            query: |
              sum(kube_pod_container_resource_requests_cpu_cores) by (pod, namespace, node)
      pod-request-memory-bytes:
        spec:
          prometheusMetricsImporter:
            query: |
              sum(kube_pod_container_resource_requests_memory_bytes) by (pod, namespace, node)
      pod-usage-cpu-cores:
        spec:
          prometheusMetricsImporter:
            query: |
              label_replace(sum(rate(container_cpu_usage_seconds_total{container_name!="POD",container_name!="",pod_name!=""}[1m])) BY (pod_name, namespace), "pod", "$1", "pod_name", "(.*)") + on (pod, namespace) group_left(node) (sum(kube_pod_info{pod_ip!="",node!="",host_ip!=""}) by (pod, namespace, node) * 0)
      pod-usage-memory-bytes:
        spec:
          prometheusMetricsImporter:
            query: |
              sum(label_replace(container_memory_usage_bytes{container_name!="POD", container_name!="",pod_name!=""}, "pod", "$1", "pod_name", "(.*)")) by (pod, namespace) + on (pod, namespace) group_left(node) (sum(kube_pod_info{pod_ip!="",node!="",host_ip!=""}) by (pod, namespace, node) * 0)

reporting-operator:
  spec:
    affinity: {}
    annotations: {}
    authProxy:
      authenticatedEmailsData: ""
      authenticatedEmailsEnabled: false
      authenticatedEmailsSecretName: reporting-operator-auth-proxy-authenticated-emails
      cookieSecretName: reporting-operator-auth-proxy-cookie-seed
      cookieSeed: ""
      createAuthProxyClusterRole: true
      createAuthenticatedEmailsSecret: true
      createCookieSecret: true
      createHtpasswdSecret: true
      delegateURLsEnabled: false
      delegateURLsPolicy: '{"/": {"group": "metering.openshift.io", "resource": "reports",
        "namespace": "$(NAMESPACE)", "subresource": "export", "verb": "get"}}'
      enabled: false
      htpasswdData: ""
      htpasswdSecretName: reporting-operator-auth-proxy-htpasswd
      image:
        pullPolicy: Always
        repository: openshift/oauth-proxy
        tag: v1.1.0
      resources:
        limits:
          cpu: 500m
          memory: 100Mi
        requests:
          cpu: 100m
          memory: 50Mi
      subjectAccessReviewEnabled: false
      subjectAccessReviewPolicy: '{"group": "metering.openshift.io", "resource": "reports",
        "namespace": "$(NAMESPACE)", "subresource": "export", "verb": "get"}'
    config:
      allNamespaces: false
      awsAccessKeyID: ""
      awsCredentialsSecretName: ""
      awsSecretAccessKey: ""
      createAwsCredentialsSecret: false
      createClusterMonitoringViewRBAC: true
      disablePrometheusMetricsImporter: "false"
      enableFinalizers: "false"
      hiveHost: hive-server:10000
      leaderLeaseDuration: 60s
      logDDLQueries: "false"
      logDMLQueries: "false"
      logLevel: info
      logReports: "false"
      prestoHost: presto:8080
      prestoMaxQueryLength: null
      prestoTLS:
        # CA for the server cert
        caCertificate: ""

        createSecret: false
        enabled: false
        secretName: ""
      prestoAuth:
        # client cert
        certificate: ""
        # client private key
        key: ""
        # CA for the client cert
        caCertificate: ""

        createSecret: false
        enabled: false
        secretName: ""
      prometheusCertificateAuthority:
        configMap:
          create: false
          enabled: false
          filename: ""
          name: reporting-operator-certificate-authority-config
          value: ""
        useServiceAccountCA: true
      prometheusDatasourceImportFrom: null
      prometheusDatasourceMaxImportBackfillDuration: null
      prometheusDatasourceMaxQueryRangeDuration: null
      prometheusImporter:
        auth:
          tokenSecret:
            create: false
            enabled: false
            name: reporting-operator-prometheus-bearer-secrets
            value: ""
          useServiceAccountToken: true
      prometheusMetricsImporterChunkSize: 5m
      prometheusMetricsImporterPollInterval: 5m
      prometheusMetricsImporterStepSize: 60s
      prometheusURL: https://prometheus-k8s.openshift-monitoring.svc:9091/
      targetNamespaces: []
      tls:
        certificateData: null
        createSecret: false
        enabled: true
        privateKeyData: null
        secretName: reporting-operator-api-tls-secrets
    image:
      pullPolicy: Always
      repository: quay.io/openshift/origin-metering-reporting-operator
      tag: 4.2
    labels: {}
    livenessProbe:
      failureThreshold: 5
      httpGet:
        path: /healthy
        port: 8080
        scheme: HTTP
      initialDelaySeconds: 120
      periodSeconds: 60
      successThreshold: 1
      timeoutSeconds: 60
    nodeSelector: {}
    readinessProbe:
      failureThreshold: 3
      httpGet:
        path: /ready
        port: 8080
        scheme: HTTP
      initialDelaySeconds: 60
      periodSeconds: 60
      successThreshold: 1
      timeoutSeconds: 60
    replicas: 1
    resources:
      limits:
        cpu: 1
        memory: 500Mi
      requests:
        cpu: 500m
        memory: 100Mi
    route:
      enabled: false
      name: metering
    service:
      annotations:
        service.alpha.openshift.io/serving-cert-secret-name: reporting-operator-api-tls-secrets
      nodePort: null
      type: ClusterIP
    tolerations: []
    updateStrategy:
      type: RollingUpdate

presto:
  spec:
    config:
      awsAccessKeyID: ""
      awsCredentialsSecretName: ""
      awsSecretAccessKey: ""
      createAwsCredentialsSecret: false

      sharedVolume:
        enabled: false
        mountPath: /user/hive/warehouse

        createPVC: false
        claimName: hive-warehouse-data
        size: 5Gi
        storageClass: null

    hive:
      annotations: {}
      config:
        autoCreateMetastoreSchema: true
        dbConnectionDriver: org.apache.derby.jdbc.EmbeddedDriver
        dbConnectionPassword: null
        dbConnectionURL: jdbc:derby:;databaseName=/var/lib/hive/data;create=true
        dbConnectionUsername: null
        defaultCompression: zlib
        defaultFileFormat: orc
        enableMetastoreSchemaVerification: false
        metastoreClientSocketTimeout: null
        metastoreURIs: thrift://hive-metastore:9083
        metastoreWarehouseDir: null
        hadoopConfigMapName: hadoop-config
        useHadoopConfigMap: true
      image:
        pullPolicy: Always
        repository: quay.io/openshift/origin-metering-hive
        tag: 4.2
      labels: {}
      metastore:
        affinity: {}
        config:
          jvm:
            initialRAMPercentage: null
            maxRAMPercentage: null
            minRAMPercentage: null
          logLevel: info
        livenessProbe:
          failureThreshold: 3
          initialDelaySeconds: 90
          periodSeconds: 30
          successThreshold: 1
          tcpSocket:
            port: 9083
          timeoutSeconds: 15
        nodeSelector: {}
        readinessProbe:
          failureThreshold: 3
          initialDelaySeconds: 60
          periodSeconds: 20
          successThreshold: 1
          tcpSocket:
            port: 9083
          timeoutSeconds: 15
        resources:
          limits:
            cpu: 4
            memory: 2Gi
          requests:
            cpu: 500m
            memory: 650Mi
        storage:
          class: null
          create: true
          size: 5Gi
        tolerations: []
      securityContext:
        fsGroup: null
        runAsNonRoot: true
      server:
        affinity: {}
        config:
          jvm:
            initialRAMPercentage: null
            maxRAMPercentage: null
            minRAMPercentage: null
          logLevel: info
        livenessProbe:
          failureThreshold: 3
          initialDelaySeconds: 300
          periodSeconds: 30
          successThreshold: 1
          tcpSocket:
            port: 10000
          timeoutSeconds: 15
        nodeSelector: {}
        readinessProbe:
          failureThreshold: 3
          initialDelaySeconds: 60
          periodSeconds: 20
          successThreshold: 1
          tcpSocket:
            port: 10000
          timeoutSeconds: 15
        resources:
          limits:
            cpu: 1
            memory: 1Gi
          requests:
            cpu: 500m
            memory: 500Mi
        tolerations: []
      terminationGracePeriodSeconds: 30
    presto:
      annotations: {}
      config:
        tls:
          # server cert
          certificate: ""
          # server private key
          key: ""
          # CA for the server cert
          caCertificate: ""

          createSecret: false
          enabled: false
          secretName: ""

        auth:
          # client cert
          certificate: ""
          # client private key
          key: ""
          # CA for the client cert
          caCertificate: ""

          createSecret: false
          enabled: false
          secretName: ""
        connectors:
          extraConnectorFiles: []
        environment: production
        hiveMetastoreURI: thrift://hive-metastore:9083
        maxQueryLength: "10000000"
        metastoreTimeout: null
        nodeSchedulerIncludeCoordinator: true
      coordinator:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - presto
              topologyKey: kubernetes.io/hostname
        config:
          jvm:
            G1HeapRegionSize: null
            concGCThreads: null
            extraFlags: []
            initiatingHeapOccupancyPercent: null
            maxGcPauseMillis: null
            parallelGCThreads: null
            initialRAMPercentage: "50.0"
            maxRAMPercentage: "50.0"
            minRAMPercentage: null
            permSize: null
            reservedCodeCacheSize: null
            maxCachedBufferSize: null
            maxDirectMemorySize: null
          logLevel: info
          taskMaxWorkerThreads: null
          taskMinDrivers: null
          logLevel: info
          taskMaxWorkerThreads: null
          taskMinDrivers: null
        nodeSelector: {}
        resources:
          limits:
            cpu: 4
            memory: 4Gi
          requests:
            cpu: 2
            memory: 2Gi
        terminationGracePeriodSeconds: 30
        tolerations: []
      image:
        pullPolicy: Always
        repository: quay.io/openshift/origin-metering-presto
        tag: 4.2
      labels: {}
      securityContext:
        fsGroup: null
        runAsNonRoot: true
      worker:
        replicas: 0
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - presto
              topologyKey: kubernetes.io/hostname
        config:
          jvm:
            G1HeapRegionSize: null
            concGCThreads: null
            extraFlags: []
            initiatingHeapOccupancyPercent: null
            maxGcPauseMillis: null
            parallelGCThreads: null
            initialRAMPercentage: "50.0"
            maxRAMPercentage: "50.0"
            minRAMPercentage: null
            permSize: null
            reservedCodeCacheSize: null
            maxCachedBufferSize: null
            maxDirectMemorySize: null
          logLevel: info
          taskMaxWorkerThreads: null
          taskMinDrivers: null
        nodeSelector: {}
        resources:
          limits:
            cpu: 8
            memory: 8Gi
          requests:
            cpu: 4
            memory: 2Gi
        terminationGracePeriodSeconds: 30
        tolerations: []

hadoop:
  spec:
    configMapName: hadoop-config

    config:
      defaultFS: hdfs://hdfs-namenode-0.hdfs-namenode:9820

      aws:
        accessKeyID: ""
        secretAccessKey: ""
        createCredentialsSecret: false
        credentialsSecretName: hadoop-aws-credentials

    image:
      pullPolicy: Always
      repository: quay.io/openshift/origin-metering-hadoop
      tag: 4.2
      pullSecrets: null

    hdfs:
      enabled: false
      config:
        datanodeDataDirPerms: "775"
        replicationFactor: 3
        logLevel: info

      datanode:
        annotations: {}
        labels: {}
        nodeSelector: {}
        tolerations: []
        terminationGracePeriodSeconds: 30
        replicas: 1

        config:
          jvm:
            initialRAMPercentage: null
            maxRAMPercentage: null
            minRAMPercentage: null

        resources:
          limits:
            cpu: 1
            memory: 1Gi
          requests:
            cpu: 250m
            memory: 500Mi

        storage:
          class: null
          size: 5Gi

        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - hdfs-datanode
              topologyKey: kubernetes.io/hostname

      namenode:
        annotations: {}
        labels: {}
        nodeSelector: {}
        terminationGracePeriodSeconds: 30
        tolerations: []

        config:
          jvm:
            initialRAMPercentage: null
            maxRAMPercentage: null
            minRAMPercentage: null

        resources:
          limits:
            cpu: 2
            memory: 2Gi
          requests:
            cpu: 500m
            memory: 500Mi

        storage:
          class: null
          size: 5Gi

        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - hdfs-namenode
              topologyKey: kubernetes.io/hostname

      securityContext:
        fsGroup: null
        runAsNonRoot: true
